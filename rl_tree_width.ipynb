{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data import Dataset, Sampler, DataLoader\n",
    "import multiprocessing as mp\n",
    "\n",
    "import gym\n",
    "import gym.spaces\n",
    "\n",
    "import copy\n",
    "\n",
    "from src.rl_environment import Environment, degree_cost\n",
    "from src.graph_model import get_peo\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print (use_cuda)\n",
    "\n",
    "def to_tensor(*args, **kwargs):\n",
    "    return torch.Tensor(*args, **kwargs).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-23 18:05:23,181- INFO•\treading file inst_2x2_7_0.txt\n",
      "2018-11-23 18:05:23,183- INFO•\tThere are 4 qubits in circuit\n",
      "2018-11-23 18:05:23,185- INFO•\tGenerated graph with 10 nodes and 22 edges\n",
      "2018-11-23 18:05:23,186- INFO•\tlast index contains from [9, 2, 5, 10]\n"
     ]
    }
   ],
   "source": [
    "env = Environment(\n",
    "    'inst_2x2_7_0.txt',\n",
    "    square_index=True,\n",
    "    cost_function=degree_cost,\n",
    "    simple_graph=True)\n",
    "state_shape = (15, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    probs = np.ones(15) / 15\n",
    "    mask = env.available_actions\n",
    "    probs = probs * mask\n",
    "    probs = probs / probs.sum()\n",
    "    \n",
    "    action = np.random.choice(15, p=probs)\n",
    "    s, cost, done = env.step(action)\n",
    "    plt.imshow(s)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.sequential import SequentialNet, name2nn\n",
    "from models.utils import create_optimal_inner_init, out_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(\n",
    "            self, output_size,\n",
    "            conv_hiddens, linear_hiddens,\n",
    "            conv_kwargs=None,\n",
    "            state_shape=[15, 15], num_channels=1,\n",
    "            activation_fn=nn.ReLU, norm_fn=None, bias=True):\n",
    "        super().__init__()\n",
    "        activation_fn = name2nn(activation_fn)\n",
    "        norm_fn = name2nn(norm_fn)\n",
    "        self.input_shape = [num_channels] + state_shape\n",
    "        default_conv_kwargs = {\n",
    "            \"kernel_size\": 3,\n",
    "            \"stride\": 2,\n",
    "            \"padding\": 0\n",
    "        }\n",
    "        conv_kwargs = conv_kwargs or default_conv_kwargs\n",
    "\n",
    "        def conv_fn(f_in, f_out, bias=True):\n",
    "            return nn.Conv2d(f_in, f_out, bias=bias, **conv_kwargs)\n",
    "\n",
    "        self.conv_net = SequentialNet(\n",
    "            hiddens=[num_channels] + conv_hiddens,\n",
    "            layer_fn=conv_fn,\n",
    "            activation_fn=activation_fn,\n",
    "            norm_fn=None,\n",
    "            bias=bias)\n",
    "        hw = state_shape[0]\n",
    "        padding = conv_kwargs.get(\"padding\", 0)\n",
    "        ksize = conv_kwargs[\"kernel_size\"]\n",
    "        stride = conv_kwargs.get(\"stride\", 1)\n",
    "        for i in range(len(conv_hiddens)):\n",
    "            hw = (hw + 2 * padding - (ksize - 1) - 1) // stride + 1\n",
    "        linear_input_size = conv_hiddens[-1] * hw ** 2\n",
    "        self.linear_net = SequentialNet(\n",
    "            hiddens=[linear_input_size] + linear_hiddens,\n",
    "            layer_fn=nn.Linear,\n",
    "            activation_fn=activation_fn,\n",
    "            norm_fn=None,\n",
    "            bias=bias)\n",
    "        self.policy_net = SequentialNet(\n",
    "            hiddens=[linear_hiddens[-1], output_size],\n",
    "            layer_fn=nn.Linear,\n",
    "            activation_fn=None,\n",
    "            norm_fn=None,\n",
    "            bias=True)\n",
    "        inner_init = create_optimal_inner_init(nonlinearity=activation_fn)\n",
    "        self.conv_net.apply(inner_init)\n",
    "        self.linear_net.apply(inner_init)\n",
    "        self.policy_net.apply(out_init)\n",
    "        \n",
    "    def forward(self, states):\n",
    "        if len(states.shape) == 3:\n",
    "            states = states[..., None]\n",
    "        x = states.permute(0, 3, 1, 2)\n",
    "        x = self.conv_net(x).view(states.shape[0], -1)\n",
    "        x = self.linear_net(x)\n",
    "        logits = self.policy_net(x)\n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(ConvNet):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def forward(self, states, mask, with_dist=False):\n",
    "        logits = super().forward(states)\n",
    "        probs = nn.functional.softmax(logits, dim=-1)\n",
    "        probs = (probs + 1e-6) * mask\n",
    "        #probs = probs * mask\n",
    "        probs = probs / probs.sum(dim=-1)[..., None]\n",
    "        dist = torch.distributions.Categorical(probs=probs)\n",
    "        action = dist.sample()\n",
    "        if with_dist:\n",
    "            return action, dist\n",
    "        return action\n",
    "\n",
    "\n",
    "class Critic(ConvNet):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def forward(self, states, with_log_pi=False):\n",
    "        values = super().forward(states)\n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(env, actor):\n",
    "    states, actions, masks, rewards, dones = [], [], [], [], []\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    worst_cost = 0\n",
    "    \n",
    "    while not done:\n",
    "        states.append(s.tolist())\n",
    "        \n",
    "        mask = env.available_actions.copy()\n",
    "\n",
    "        a, dist = actor.forward(\n",
    "            to_tensor(s[None, :, :, None]),\n",
    "            mask=to_tensor(mask[None, :]),\n",
    "            with_dist=True)\n",
    "        a = a.cpu().detach().numpy()[0]\n",
    "\n",
    "        s, r, done = env.step(a)\n",
    "        \n",
    "        worst_cost = max(worst_cost, r)\n",
    "        \n",
    "        if done:\n",
    "            reward = -worst_cost\n",
    "        else:\n",
    "            reward = 0\n",
    "        \n",
    "        actions.append(a)\n",
    "        masks.append(mask.tolist())\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "\n",
    "    return states, actions, masks, rewards, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_shape = (15, 15)\n",
    "conv_hiddens=[32, 64, 64]\n",
    "linear_hiddens = [256, 128]\n",
    "\n",
    "actor = Actor(\n",
    "    output_size=15,\n",
    "    conv_hiddens=conv_hiddens,\n",
    "    linear_hiddens=linear_hiddens).to(device) \n",
    "critic = Critic(\n",
    "    output_size=1,\n",
    "    conv_hiddens=conv_hiddens,\n",
    "    linear_hiddens=linear_hiddens).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BufferDataset(Dataset):\n",
    "    def __init__(self, state_shape, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.len = 0\n",
    "        self.pointer = 0\n",
    "        \n",
    "        self.states = np.empty((self.max_size,) + state_shape, dtype=np.float32)\n",
    "        self.actions = np.empty((self.max_size,), dtype=np.int32)\n",
    "        self.masks = np.empty((self.max_size, 15), dtype=np.int32)\n",
    "        self.returns = np.empty((self.max_size,), dtype=np.float32)\n",
    "        self.values = np.empty((self.max_size,), dtype=np.float32)\n",
    "        self.advantages = np.empty((self.max_size,), dtype=np.float32)\n",
    "        self.log_pis = np.empty((self.max_size,), dtype=np.float32)\n",
    "\n",
    "    def push_episode(self, episode):\n",
    "        states, actions, masks, ret, val, adv, log_pi = episode\n",
    "        episode_len = len(actions)\n",
    "        self.len = min(self.len + episode_len, self.max_size)\n",
    "        indices = np.arange(\n",
    "            self.pointer, self.pointer + episode_len) % self.max_size\n",
    "        self.states[indices] = states\n",
    "        self.actions[indices] = actions\n",
    "        self.masks[indices] = masks\n",
    "        self.returns[indices] = ret\n",
    "        self.values[indices] = val\n",
    "        self.advantages[indices] = adv\n",
    "        self.log_pis[indices] = log_pi\n",
    "        self.pointer = (self.pointer + episode_len) % self.max_size\n",
    "\n",
    "    def rescale_advantages(self):\n",
    "        adv_centered = self.advantages[:self.len] - self.advantages[:self.len].mean()\n",
    "        self.advantages[:self.len] = adv_centered / (self.advantages[:self.len].std() + 1e-6)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        dct = {\n",
    "            \"state\": self.states[index],\n",
    "            \"action\": self.actions[index].astype(np.float32),\n",
    "            \"mask\": self.masks[index].astype(np.float32),\n",
    "            \"return\": self.returns[index],\n",
    "            \"value\": self.values[index],\n",
    "            \"advantage\": self.advantages[index],\n",
    "            \"log_pi\": self.log_pis[index]\n",
    "        }\n",
    "        return dct\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "        \n",
    "        \n",
    "class BufferSampler(Sampler):\n",
    "    def __init__(self, buffer, num_mini_epochs):\n",
    "        super().__init__(None)\n",
    "        self.buffer = buffer\n",
    "        self.num_mini_epochs = num_mini_epochs\n",
    "        buffer_len = len(self.buffer)\n",
    "        self.len = buffer_len * num_mini_epochs\n",
    "\n",
    "        indices = []\n",
    "        for i in range(num_mini_epochs):\n",
    "            idx = np.arange(buffer_len)\n",
    "            np.random.shuffle(idx)\n",
    "            indices.append(idx)\n",
    "        self.indices = np.concatenate(indices)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gamma_matrix(tau, matrix_size):\n",
    "    \"\"\"\n",
    "    Matrix of the following form\n",
    "    --------------------\n",
    "    1     y   y^2    y^3\n",
    "    0     1     y    y^2\n",
    "    0     0     1      y\n",
    "    0     0     0      1\n",
    "    --------------------\n",
    "    for fast gae calculation\n",
    "    \"\"\"\n",
    "    i = np.arange(matrix_size)\n",
    "    j = np.arange(matrix_size)\n",
    "    pow_ = i[None, :] - j[:, None]\n",
    "    mat = np.power(tau, pow_) * (pow_ >= 0)\n",
    "    return mat\n",
    "\n",
    "\n",
    "class PPO_GAE:\n",
    "    def __init__(\n",
    "            self,\n",
    "            actor, critic,\n",
    "            gamma, gae_lambda=0.95,\n",
    "            clip_eps=0.2,\n",
    "            episode_len=1000,\n",
    "            minibatch_size=32,\n",
    "            num_mini_epochs=10,\n",
    "            use_value_clipping=False,\n",
    "            entropy_reg_coefficient=0.):\n",
    "        self._device = device\n",
    "\n",
    "        self.actor = actor.to(self._device)\n",
    "        self.critic = critic.to(self._device)\n",
    "        \n",
    "        self.actor_optimizer = optim.Adam(\n",
    "            self.actor.parameters(), lr=lr)\n",
    "        self.critic_optimizer = optim.Adam(\n",
    "            self.critic.parameters(), lr=lr)\n",
    "\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.gamma = gamma\n",
    "        self.clip_eps = clip_eps\n",
    "        self.episode_len = episode_len\n",
    "        self.num_mini_epochs = num_mini_epochs\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.use_value_clipping = use_value_clipping\n",
    "        self.entropy_reg_coefficient = entropy_reg_coefficient\n",
    "\n",
    "        self.gam_lam_matrix = create_gamma_matrix(\n",
    "            self.gamma * gae_lambda, episode_len)\n",
    "        self.gam_matrix = create_gamma_matrix(\n",
    "            self.gamma, episode_len)\n",
    "        \n",
    "    def to_tensor(self, *args, **kwargs):\n",
    "        return torch.Tensor(*args, **kwargs).to(self._device)\n",
    "\n",
    "    def evaluate_episode(self, states, actions, masks, rewards):\n",
    "        states = self.to_tensor(states)\n",
    "        actions = self.to_tensor(actions)\n",
    "        masks = self.to_tensor(masks)\n",
    "        rewards = np.array(rewards)\n",
    "\n",
    "        ep_len = rewards.shape[0]\n",
    "        values = torch.zeros((ep_len + 1, 1)).to(self._device)\n",
    "        values[:ep_len] = self.critic(states)\n",
    "        values = values.detach().cpu().numpy().reshape(-1)\n",
    "        \n",
    "        _, dist = self.actor(states, masks, with_dist=True)\n",
    "        log_pis = dist.log_prob(actions)\n",
    "        log_pis = log_pis.detach().cpu().numpy().reshape(-1)\n",
    "        \n",
    "        deltas = rewards + self.gamma * values[1:] - values[:-1]\n",
    "        advantages = np.dot(self.gam_lam_matrix[:ep_len, :ep_len], deltas)\n",
    "        returns = np.dot(self.gam_matrix[:ep_len, :ep_len], rewards)\n",
    "\n",
    "        return [returns, values[:ep_len], advantages, log_pis]\n",
    "\n",
    "    def train(self, batch):\n",
    "        states, actions, masks, returns, values, advantages, log_pis = \\\n",
    "            batch[\"state\"], batch[\"action\"], batch[\"mask\"], batch[\"return\"], \\\n",
    "            batch[\"value\"], batch[\"advantage\"], batch[\"log_pi\"]\n",
    "\n",
    "        states = self.to_tensor(states)\n",
    "        actions = self.to_tensor(actions)\n",
    "        masks = self.to_tensor(masks)\n",
    "        returns = self.to_tensor(returns)\n",
    "        old_values = self.to_tensor(values)\n",
    "        advantages = self.to_tensor(advantages)\n",
    "        old_log_pi = self.to_tensor(log_pis)\n",
    "\n",
    "        values_t = self.critic(states).squeeze()\n",
    "        if self.use_value_clipping:\n",
    "            values_clip = old_values + torch.clamp(\n",
    "                values_t - old_values, -self.clip_eps, self.clip_eps)\n",
    "            val_loss1 = (values_t - returns).pow(2)\n",
    "            val_loss2 = (values_clip - returns).pow(2)\n",
    "            value_loss = 0.5 * torch.max(val_loss1, val_loss2).mean()\n",
    "        else:\n",
    "            value_loss = 0.5 * (values_t - returns).pow(2).mean()\n",
    "\n",
    "        # actor loss\n",
    "        _, dist = self.actor(states, masks, with_dist=True)\n",
    "        log_pi = dist.log_prob(actions)\n",
    "\n",
    "        ratio = torch.exp(log_pi - old_log_pi)\n",
    "        surr1 = advantages * ratio\n",
    "        surr2 = advantages * torch.clamp(\n",
    "            ratio, 1.0 - self.clip_eps, 1.0 + self.clip_eps)\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "        entropy = -(torch.exp(log_pi) * log_pi).mean()\n",
    "        entropy_reg = self.entropy_reg_coefficient * entropy\n",
    "        policy_loss = policy_loss + entropy_reg\n",
    "\n",
    "        # actor update\n",
    "        self.actor_update(policy_loss)\n",
    "        # critic update\n",
    "        self.critic_update(value_loss)\n",
    "\n",
    "        metrics = {\n",
    "            \"loss_actor\": policy_loss.item(),\n",
    "            \"loss_critic\": value_loss.item()\n",
    "        }\n",
    "        return metrics\n",
    "        \n",
    "    def actor_update(self, loss):\n",
    "        self.actor.zero_grad()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "    def critic_update(self, loss):\n",
    "        self.critic.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.critic_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "lr = 1e-3\n",
    "\n",
    "num_mini_epochs = 6\n",
    "minibatch_size = 500\n",
    "\n",
    "state_shape = (15, 15)\n",
    "conv_hiddens=[64, 64]\n",
    "linear_hiddens = [128, 128]\n",
    "\n",
    "actor = Actor(\n",
    "    output_size=15,\n",
    "    conv_hiddens=conv_hiddens,\n",
    "    linear_hiddens=linear_hiddens).to(device) \n",
    "critic = Critic(\n",
    "    output_size=1,\n",
    "    conv_hiddens=conv_hiddens,\n",
    "    linear_hiddens=linear_hiddens).to(device)\n",
    "\n",
    "algo = PPO_GAE(\n",
    "    actor, critic,\n",
    "    gamma=0.99, gae_lambda=0.95,\n",
    "    minibatch_size=minibatch_size,\n",
    "    num_mini_epochs=num_mini_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor loss:          -0.01599576718096311\n",
      "Critic loss:         4.47100751598676\n",
      "Average reward:      -4.35\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.028078038347302936\n",
      "Critic loss:         1.1952178180217743\n",
      "Average reward:      -4.295\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.0332979634598208\n",
      "Critic loss:         0.37156981726487476\n",
      "Average reward:      -4.23\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04163238485731805\n",
      "Critic loss:         0.25417792921264964\n",
      "Average reward:      -4.39\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04605538349521036\n",
      "Critic loss:         0.22454598111410937\n",
      "Average reward:      -4.25\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04813751626837378\n",
      "Critic loss:         0.22107603028416634\n",
      "Average reward:      -4.3\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.047659304070596896\n",
      "Critic loss:         0.2387179583311081\n",
      "Average reward:      -4.305\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.05286780015255014\n",
      "Critic loss:         0.2282488097747167\n",
      "Average reward:      -4.23\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.0458858065539971\n",
      "Critic loss:         0.2633381796379884\n",
      "Average reward:      -4.35\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.054113277758006006\n",
      "Critic loss:         0.21067666014035544\n",
      "Average reward:      -4.275\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04945980091967309\n",
      "Critic loss:         0.19699629520376524\n",
      "Average reward:      -4.2\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.049993151512656674\n",
      "Critic loss:         0.2201716030637423\n",
      "Average reward:      -4.34\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04783383070025593\n",
      "Critic loss:         0.21997895402212939\n",
      "Average reward:      -4.195\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.05133881208287979\n",
      "Critic loss:         0.23490290778378645\n",
      "Average reward:      -4.245\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.0474073623578685\n",
      "Critic loss:         0.20286107622087002\n",
      "Average reward:      -4.255\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.0499891316430876\n",
      "Critic loss:         0.20395917197068533\n",
      "Average reward:      -4.17\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04975691636597427\n",
      "Critic loss:         0.24358273421724638\n",
      "Average reward:      -4.185\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.05382301568715775\n",
      "Critic loss:         0.2372300811111927\n",
      "Average reward:      -4.1\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04759457612332577\n",
      "Critic loss:         0.19781410011152425\n",
      "Average reward:      -4.12\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.049951285570083805\n",
      "Critic loss:         0.24109123647212982\n",
      "Average reward:      -4.145\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04962955320176358\n",
      "Critic loss:         0.19432333608468375\n",
      "Average reward:      -4.18\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.05149770227338498\n",
      "Critic loss:         0.2670154329389334\n",
      "Average reward:      -4.105\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04763815567518274\n",
      "Critic loss:         0.19383320026099682\n",
      "Average reward:      -4.1\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04671967160053706\n",
      "Critic loss:         0.18216278962790966\n",
      "Average reward:      -4.195\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04605201105490172\n",
      "Critic loss:         0.19787805465360483\n",
      "Average reward:      -4.125\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.046840654889820144\n",
      "Critic loss:         0.2627052130798499\n",
      "Average reward:      -4.145\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.040679379793194435\n",
      "Critic loss:         0.18819491813580194\n",
      "Average reward:      -4.02\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04636514517793936\n",
      "Critic loss:         0.20798823175330958\n",
      "Average reward:      -4.045\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.040493972677116595\n",
      "Critic loss:         0.1976128232975801\n",
      "Average reward:      -3.97\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.03435079472061867\n",
      "Critic loss:         0.18038365679482618\n",
      "Average reward:      -4.08\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.03918873810228737\n",
      "Critic loss:         0.1769868191331625\n",
      "Average reward:      -4.045\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.042751530456977584\n",
      "Critic loss:         0.23983541255195936\n",
      "Average reward:      -4.045\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.041776468972481474\n",
      "Critic loss:         0.19671069271862507\n",
      "Average reward:      -3.935\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.0426489139451102\n",
      "Critic loss:         0.2082715599487225\n",
      "Average reward:      -3.975\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.042547350322517254\n",
      "Critic loss:         0.18878443104525408\n",
      "Average reward:      -3.93\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.040146769516771506\n",
      "Critic loss:         0.20027973564962545\n",
      "Average reward:      -3.96\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04055677482392639\n",
      "Critic loss:         0.18594820300738016\n",
      "Average reward:      -4.0\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.03919815235227967\n",
      "Critic loss:         0.20505190330247083\n",
      "Average reward:      -3.955\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.040105780402276046\n",
      "Critic loss:         0.19543460694452128\n",
      "Average reward:      -3.925\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.040034279809333384\n",
      "Critic loss:         0.1825329065322876\n",
      "Average reward:      -3.99\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.039205780592358984\n",
      "Critic loss:         0.1979520358145237\n",
      "Average reward:      -3.885\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.03820739215128318\n",
      "Critic loss:         0.16988777928054333\n",
      "Average reward:      -3.915\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.0367570566474266\n",
      "Critic loss:         0.1733208242803812\n",
      "Average reward:      -3.9\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.039212663541547954\n",
      "Critic loss:         0.19614179556568465\n",
      "Average reward:      -3.99\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04543285114535441\n",
      "Critic loss:         0.21739628973106542\n",
      "Average reward:      -3.9\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.039466382266255096\n",
      "Critic loss:         0.20035311145087084\n",
      "Average reward:      -3.825\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor loss:          -0.03730564191209851\n",
      "Critic loss:         0.1902521817634503\n",
      "Average reward:      -3.85\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.03475969212983424\n",
      "Critic loss:         0.17628242137531439\n",
      "Average reward:      -3.89\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.041708234328931816\n",
      "Critic loss:         0.18023699397842088\n",
      "Average reward:      -3.9\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04048633294587489\n",
      "Critic loss:         0.18475097542007765\n",
      "Average reward:      -3.885\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04032369607981915\n",
      "Critic loss:         0.17987725945810476\n",
      "Average reward:      -3.925\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.039581250166520476\n",
      "Critic loss:         0.15466239800055823\n",
      "Average reward:      -3.845\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04128626194627335\n",
      "Critic loss:         0.18190859692792097\n",
      "Average reward:      -3.85\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.03965560069385295\n",
      "Critic loss:         0.17626266243557134\n",
      "Average reward:      -3.845\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04054223999749714\n",
      "Critic loss:         0.18970409967005253\n",
      "Average reward:      -3.83\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.03840223220807578\n",
      "Critic loss:         0.19340359792113304\n",
      "Average reward:      -3.815\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.039906961067269243\n",
      "Critic loss:         0.1987575888633728\n",
      "Average reward:      -3.77\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04204750535427593\n",
      "Critic loss:         0.17087053321301937\n",
      "Average reward:      -3.71\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04118699146783911\n",
      "Critic loss:         0.20950299377242723\n",
      "Average reward:      -3.805\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.038982775697756246\n",
      "Critic loss:         0.1935131543626388\n",
      "Average reward:      -3.765\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04066393251317398\n",
      "Critic loss:         0.16344691440463066\n",
      "Average reward:      -3.735\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.03672239884811764\n",
      "Critic loss:         0.17390815851589045\n",
      "Average reward:      -3.88\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04235551386955194\n",
      "Critic loss:         0.20539230667054653\n",
      "Average reward:      -3.795\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.035510467931696134\n",
      "Critic loss:         0.18433935878177485\n",
      "Average reward:      -3.85\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.03525964714935981\n",
      "Critic loss:         0.17997539850572744\n",
      "Average reward:      -3.86\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.03825178087087503\n",
      "Critic loss:         0.20419144940872988\n",
      "Average reward:      -3.82\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.03591945200363019\n",
      "Critic loss:         0.19008141631881395\n",
      "Average reward:      -3.835\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04208111669868231\n",
      "Critic loss:         0.17159705546995005\n",
      "Average reward:      -3.725\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.037738002783347234\n",
      "Critic loss:         0.18035189931591353\n",
      "Average reward:      -3.805\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04115620855009183\n",
      "Critic loss:         0.1957804678628842\n",
      "Average reward:      -3.77\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04305738799545603\n",
      "Critic loss:         0.15929308471580347\n",
      "Average reward:      -3.79\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.039963794847911537\n",
      "Critic loss:         0.20389442642529806\n",
      "Average reward:      -3.815\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.041801295592449605\n",
      "Critic loss:         0.18878699031968912\n",
      "Average reward:      -3.74\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.03593878311706552\n",
      "Critic loss:         0.1820789035409689\n",
      "Average reward:      -3.8\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04434675277055552\n",
      "Critic loss:         0.19408167091508707\n",
      "Average reward:      -3.725\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04045788338407874\n",
      "Critic loss:         0.18493727842966715\n",
      "Average reward:      -3.83\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04108607609911511\n",
      "Critic loss:         0.20552131347358227\n",
      "Average reward:      -3.765\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.038290667541635535\n",
      "Critic loss:         0.17069586304326853\n",
      "Average reward:      -3.765\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.03911334778240416\n",
      "Critic loss:         0.18794227639834085\n",
      "Average reward:      -3.815\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.040236707388733826\n",
      "Critic loss:         0.18124478061993918\n",
      "Average reward:      -3.715\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.041398013466581084\n",
      "Critic loss:         0.17760544766982397\n",
      "Average reward:      -3.685\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.03720160816252852\n",
      "Critic loss:         0.18701354352136454\n",
      "Average reward:      -3.755\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.03841520586865954\n",
      "Critic loss:         0.18678098606566587\n",
      "Average reward:      -3.815\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.03947505780767339\n",
      "Critic loss:         0.16983564011752605\n",
      "Average reward:      -3.7\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.038904175657080486\n",
      "Critic loss:         0.19944223885734877\n",
      "Average reward:      -3.765\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.038964923073460035\n",
      "Critic loss:         0.16549473566313586\n",
      "Average reward:      -3.7\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.0361090175845978\n",
      "Critic loss:         0.17984885474046072\n",
      "Average reward:      -3.715\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.039485078164337516\n",
      "Critic loss:         0.16557084334393343\n",
      "Average reward:      -3.71\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04108220090468725\n",
      "Critic loss:         0.1875336648275455\n",
      "Average reward:      -3.66\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.040410749434765116\n",
      "Critic loss:         0.20372872365017733\n",
      "Average reward:      -3.765\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04297278200586637\n",
      "Critic loss:         0.17315308811763921\n",
      "Average reward:      -3.735\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04096767528487059\n",
      "Critic loss:         0.1645264457911253\n",
      "Average reward:      -3.7\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor loss:          -0.04204862062700462\n",
      "Critic loss:         0.1722795528670152\n",
      "Average reward:      -3.72\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.03755333440252192\n",
      "Critic loss:         0.1703268624842167\n",
      "Average reward:      -3.655\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04424790322082117\n",
      "Critic loss:         0.1779735709230105\n",
      "Average reward:      -3.695\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04385573599332323\n",
      "Critic loss:         0.1667662262916565\n",
      "Average reward:      -3.69\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.038632157627337925\n",
      "Critic loss:         0.1611222835878531\n",
      "Average reward:      -3.635\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.03966642855812097\n",
      "Critic loss:         0.17656001138190427\n",
      "Average reward:      -3.71\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.041227947493704654\n",
      "Critic loss:         0.15234726232786974\n",
      "Average reward:      -3.605\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04274275392648027\n",
      "Critic loss:         0.16742933293183646\n",
      "Average reward:      -3.685\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04177023743007643\n",
      "Critic loss:         0.1832455483575662\n",
      "Average reward:      -3.66\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.03942227409182427\n",
      "Critic loss:         0.17839255121846995\n",
      "Average reward:      -3.745\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04111880960408598\n",
      "Critic loss:         0.24280892436703047\n",
      "Average reward:      -3.75\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.040552309297102816\n",
      "Critic loss:         0.17084468404452005\n",
      "Average reward:      -3.655\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04049716755980626\n",
      "Critic loss:         0.19644019504388174\n",
      "Average reward:      -3.685\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04196402999029184\n",
      "Critic loss:         0.16728157363831997\n",
      "Average reward:      -3.665\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.0373403076082468\n",
      "Critic loss:         0.19523716097076735\n",
      "Average reward:      -3.745\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.0440419867906409\n",
      "Critic loss:         0.17716044063369432\n",
      "Average reward:      -3.595\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04251018432357038\n",
      "Critic loss:         0.17635135600964227\n",
      "Average reward:      -3.615\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04033992163992176\n",
      "Critic loss:         0.18902709210912386\n",
      "Average reward:      -3.675\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.041078959601388\n",
      "Critic loss:         0.1609586210300525\n",
      "Average reward:      -3.605\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04185414970925194\n",
      "Critic loss:         0.14950293861329556\n",
      "Average reward:      -3.725\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04464534254899869\n",
      "Critic loss:         0.17914079378048578\n",
      "Average reward:      -3.64\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.03904770877367506\n",
      "Critic loss:         0.17912518543501696\n",
      "Average reward:      -3.63\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.043288999111003555\n",
      "Critic loss:         0.19093897504111132\n",
      "Average reward:      -3.645\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.03874622760728622\n",
      "Critic loss:         0.20311318710446358\n",
      "Average reward:      -3.64\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04004450573120266\n",
      "Critic loss:         0.17261847543219724\n",
      "Average reward:      -3.66\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04312583740102127\n",
      "Critic loss:         0.16110031865537167\n",
      "Average reward:      -3.61\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04049990003598699\n",
      "Critic loss:         0.18437916599214077\n",
      "Average reward:      -3.68\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04024598642718047\n",
      "Critic loss:         0.16723861545324326\n",
      "Average reward:      -3.62\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.042186216868382566\n",
      "Critic loss:         0.16733264364302158\n",
      "Average reward:      -3.59\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.03787342458963394\n",
      "Critic loss:         0.17214358660082021\n",
      "Average reward:      -3.595\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04039451190328691\n",
      "Critic loss:         0.16176332471271357\n",
      "Average reward:      -3.525\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04292328724598823\n",
      "Critic loss:         0.15181381131211916\n",
      "Average reward:      -3.59\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.0415539499129712\n",
      "Critic loss:         0.21541414161523184\n",
      "Average reward:      -3.565\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.044720729269708194\n",
      "Critic loss:         0.17904997679094473\n",
      "Average reward:      -3.615\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.0415339952839228\n",
      "Critic loss:         0.16433075877527395\n",
      "Average reward:      -3.615\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.039523380381676056\n",
      "Critic loss:         0.1933487101147572\n",
      "Average reward:      -3.585\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04715994819222639\n",
      "Critic loss:         0.15873709445198378\n",
      "Average reward:      -3.56\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04225078591844067\n",
      "Critic loss:         0.1891600377857685\n",
      "Average reward:      -3.615\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.03994717390560254\n",
      "Critic loss:         0.1726322453469038\n",
      "Average reward:      -3.66\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04263356856730146\n",
      "Critic loss:         0.18007582674423853\n",
      "Average reward:      -3.66\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04218961424582327\n",
      "Critic loss:         0.16627292955915132\n",
      "Average reward:      -3.525\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.041448216031615935\n",
      "Critic loss:         0.16827481240034103\n",
      "Average reward:      -3.55\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04378475962827603\n",
      "Critic loss:         0.17949538367489973\n",
      "Average reward:      -3.56\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04030163106654072\n",
      "Critic loss:         0.15882560797035694\n",
      "Average reward:      -3.59\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04217484219892261\n",
      "Critic loss:         0.19904043277104697\n",
      "Average reward:      -3.555\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04083611232150967\n",
      "Critic loss:         0.16393793125947317\n",
      "Average reward:      -3.57\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor loss:          -0.03943716030335054\n",
      "Critic loss:         0.20470960500339666\n",
      "Average reward:      -3.605\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.03925007799261948\n",
      "Critic loss:         0.1957172124336163\n",
      "Average reward:      -3.68\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04081622609131349\n",
      "Critic loss:         0.18565550446510315\n",
      "Average reward:      -3.66\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.0449445455839547\n",
      "Critic loss:         0.16699539745847383\n",
      "Average reward:      -3.63\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04189424593156824\n",
      "Critic loss:         0.16444436026116213\n",
      "Average reward:      -3.6\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.041080796315024294\n",
      "Critic loss:         0.1705947226534287\n",
      "Average reward:      -3.6\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04185755530003613\n",
      "Critic loss:         0.1423121994982163\n",
      "Average reward:      -3.57\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04260241579807674\n",
      "Critic loss:         0.16329817411800227\n",
      "Average reward:      -3.615\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04367444938786017\n",
      "Critic loss:         0.1717241139461597\n",
      "Average reward:      -3.665\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.0423366675095167\n",
      "Critic loss:         0.18729056728382906\n",
      "Average reward:      -3.61\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.042344251240137964\n",
      "Critic loss:         0.19850859480599561\n",
      "Average reward:      -3.655\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.039729353156872094\n",
      "Critic loss:         0.2034869653483232\n",
      "Average reward:      -3.64\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.044059621021006024\n",
      "Critic loss:         0.16532822387913862\n",
      "Average reward:      -3.585\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04457591502189947\n",
      "Critic loss:         0.1880811961988608\n",
      "Average reward:      -3.675\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04209555556493191\n",
      "Critic loss:         0.1995965080956618\n",
      "Average reward:      -3.555\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.040866206031447895\n",
      "Critic loss:         0.15295876686771712\n",
      "Average reward:      -3.515\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04238829412982644\n",
      "Critic loss:         0.18418236138919988\n",
      "Average reward:      -3.62\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04166632146954422\n",
      "Critic loss:         0.16561136208474636\n",
      "Average reward:      -3.55\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.042983286296172686\n",
      "Critic loss:         0.15518577148516974\n",
      "Average reward:      -3.54\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.039218101480704114\n",
      "Critic loss:         0.20190792717039585\n",
      "Average reward:      -3.61\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04052159884789338\n",
      "Critic loss:         0.1618128251284361\n",
      "Average reward:      -3.56\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04065868892939761\n",
      "Critic loss:         0.1872138058145841\n",
      "Average reward:      -3.555\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04291972765349783\n",
      "Critic loss:         0.18123473537464937\n",
      "Average reward:      -3.575\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.040103899606037885\n",
      "Critic loss:         0.16811094246804714\n",
      "Average reward:      -3.5\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04037275692098774\n",
      "Critic loss:         0.16620640084147453\n",
      "Average reward:      -3.47\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.0435614126715033\n",
      "Critic loss:         0.17749196911851564\n",
      "Average reward:      -3.52\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04166210892920693\n",
      "Critic loss:         0.17472214562197527\n",
      "Average reward:      -3.595\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.043505352286350295\n",
      "Critic loss:         0.2021275715281566\n",
      "Average reward:      -3.585\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.041757292120261504\n",
      "Critic loss:         0.15361100931962332\n",
      "Average reward:      -3.525\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04371776598660896\n",
      "Critic loss:         0.16641289119919142\n",
      "Average reward:      -3.53\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.03892486053518951\n",
      "Critic loss:         0.17143326128522554\n",
      "Average reward:      -3.53\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04094143480566951\n",
      "Critic loss:         0.15463118130962053\n",
      "Average reward:      -3.555\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.042476547843155764\n",
      "Critic loss:         0.19330193164447942\n",
      "Average reward:      -3.585\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04311071877600625\n",
      "Critic loss:         0.1893551324804624\n",
      "Average reward:      -3.585\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04019235978679111\n",
      "Critic loss:         0.1668311400959889\n",
      "Average reward:      -3.47\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n",
      "Actor loss:          -0.04182981423461266\n",
      "Critic loss:         0.17853549681603909\n",
      "Average reward:      -3.55\n",
      "Num episodes:        200\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/opensim-rl/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mndim\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   2682\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2683\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2684\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'ndim'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8353b51e4faa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mepisode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         ret, val, adv, log_pi = algo.evaluate_episode(\n\u001b[1;32m     18\u001b[0m             episode[0], episode[1], episode[2], episode[3])\n",
      "\u001b[0;32m<ipython-input-8-a41023ab8d00>\u001b[0m in \u001b[0;36mplay_episode\u001b[0;34m(env, actor)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mworst_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworst_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/qtree/src/rl_environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    242\u001b[0m         adj_matrix = np.asarray(\n\u001b[1;32m    243\u001b[0m             sparse_graph_adjacency(\n\u001b[0;32m--> 244\u001b[0;31m                 self.graph, MAX_STATE_SIZE, self.node_to_row).todense()\n\u001b[0m\u001b[1;32m    245\u001b[0m         )\n\u001b[1;32m    246\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madj_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtril_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/qtree/src/rl_environment.py\u001b[0m in \u001b[0;36msparse_graph_adjacency\u001b[0;34m(G, max_size, node_to_row, weight)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdiag_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdiag_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/opensim-rl/lib/python3.6/site-packages/scipy/sparse/coo.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0misshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m                 \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/opensim-rl/lib/python3.6/site-packages/scipy/sparse/sputils.py\u001b[0m in \u001b[0;36misshape\u001b[0;34m(x, nonneg)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misintlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misintlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnonneg\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/opensim-rl/lib/python3.6/site-packages/scipy/sparse/sputils.py\u001b[0m in \u001b[0;36misintlike\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;31m# Fast-path check to eliminate non-scalar values. operator.index would\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;31m# catch this case too, but the exception catching is slow.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/opensim-rl/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mndim\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   2683\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 0\n",
    "avg_reward = 0\n",
    "sum_actor_loss = 0\n",
    "sum_critic_loss = 0\n",
    "\n",
    "actor_losses = []\n",
    "critic_losses = []\n",
    "total_rewards = []\n",
    "episode_rewards = []\n",
    "\n",
    "for epoch in range(1, 10000):\n",
    "    \n",
    "    buffer = BufferDataset(state_shape, batch_size)\n",
    "\n",
    "    while len(buffer) < batch_size:\n",
    "        episode = play_episode(env, algo.actor)\n",
    "        ret, val, adv, log_pi = algo.evaluate_episode(\n",
    "            episode[0], episode[1], episode[2], episode[3])\n",
    "        data = [episode[0], episode[1], episode[2], ret, val, adv, log_pi]\n",
    "        buffer.push_episode(data)\n",
    "        episode_rewards.append(np.sum(episode[3]))\n",
    "        num_episodes += 1\n",
    "    buffer.rescale_advantages()\n",
    "        \n",
    "    sampler = BufferSampler(\n",
    "        buffer=buffer,\n",
    "        num_mini_epochs=num_mini_epochs)\n",
    "    loader = DataLoader(\n",
    "        dataset=buffer,\n",
    "        batch_size=minibatch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        sampler=sampler)\n",
    "        \n",
    "    actor_loss = []\n",
    "    critic_loss = []\n",
    "    for i, batch in enumerate(loader):\n",
    "        metrics = algo.train(batch)\n",
    "        actor_loss.append(metrics[\"loss_actor\"])\n",
    "        critic_loss.append(metrics[\"loss_critic\"])\n",
    "    sum_actor_loss += np.mean(actor_loss)\n",
    "    sum_critic_loss += np.mean(critic_loss)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        avg_rew = np.mean(episode_rewards)\n",
    "        avg_actor_loss = sum_actor_loss\n",
    "        avg_critic_loss = sum_critic_loss\n",
    "        print (\"Actor loss:         \", avg_actor_loss)\n",
    "        print (\"Critic loss:        \", avg_critic_loss)\n",
    "        print (\"Average reward:     \", avg_rew)\n",
    "        print (\"Num episodes:       \", num_episodes)\n",
    "        print (\"--------------------------------------------\")\n",
    "        actor_losses.append(avg_actor_loss)\n",
    "        critic_losses.append(avg_critic_loss)\n",
    "        total_rewards.append(avg_rew)\n",
    "        sum_actor_loss = 0\n",
    "        sum_critic_loss = 0\n",
    "        episode_rewards = []\n",
    "    num_episodes = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0b9451cd68>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(critic_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0b953a5c88>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-23 18:17:51,893- INFO•\treading file inst_2x2_7_0.txt\n",
      "2018-11-23 18:17:51,894- INFO•\tThere are 4 qubits in circuit\n",
      "2018-11-23 18:17:51,896- INFO•\tGenerated graph with 10 nodes and 22 edges\n",
      "2018-11-23 18:17:51,896- INFO•\tlast index contains from [9, 2, 5, 10]\n"
     ]
    }
   ],
   "source": [
    "env = Environment(\n",
    "    'inst_2x2_7_0.txt',\n",
    "    square_index=True,\n",
    "    cost_function=degree_cost,\n",
    "    simple_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-23 18:17:57,184- INFO•\tgenerating config file output/quickbb.cnf\n",
      "2018-11-23 18:17:57,187- INFO•\texcecuting quickbb: ./quickbb/quickbb_64 --min-fill-ordering --time 60 --cnffile output/quickbb.cnf \n",
      "2018-11-23 18:17:57,282- INFO•\tb'\\n8 4 3 9 6 10 7 5 2 1 \\nTreewidth= 3\\n'\n",
      "2018-11-23 18:17:57,285- INFO•\tFinal peo from quickBB:\n",
      "[8, 4, 3, 9, 6, 10, 7, 5, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "l, tw = get_peo(env.graph, )\n",
    "actions = [env.node_to_row[node] for node in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env1 = copy.deepcopy(env)\n",
    "env2 = copy.deepcopy(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAADExJREFUeJzt3X2sZPVdx/H3RxZY2SIsYilliYAhJNiYQDZIa1MbV5EiYftH/1hiFUoT0pgqmCbNIolN/Ku1pj7ExmYDVYwEGilY0oBlpW2MiayF7fK4FLaIsOvyoDXQh1jY+PWPOdtcrnN3LzPnnJ31934lkzkz5zf3fPfM/dzzMGfnm6pCUnt+7EgXIOnIMPxSowy/1CjDLzXK8EuNMvxSowy/1CjDLzXK8EuNWjPmwo7L8bWWdWMuUmrKf/N9XqsfZjVjRw3/Wtbx89k05iKlpuyo+1c91t1+qVGGX2rUXOFPcmmSbyXZk2RrX0VJGt7M4U9yDPBZ4H3A+cCVSc7vqzBJw5pny38RsKeqnqmq14Dbgc39lCVpaPOE/wzg+SWP93bPSToKDP5RX5JrgWsB1nLC0IuTtErzbPn3AWcuebyhe+4NqmpbVW2sqo3Hcvwci5PUp3nC/w3g3CRnJzkO2ALc3U9ZkoY2825/VR1I8lHgK8AxwOer6vHeKpM0qLmO+avqHuCenmqRNCKv8JMaZfilRhl+qVGGX2qU4ZcaZfilRhl+qVGGX2qU4ZcaZfilRhl+qVGGX2qU4ZcaZfilRhl+qVGGX2qU4ZcaZfilRhl+qVGGX2rUPL36zkzytSRPJHk8yXV9FiZpWPN8e+8B4GNVtTPJicBDSbZX1RM91SZpQDNv+atqf1Xt7Ka/C+zGXn3SUaOXY/4kZwEXADv6+HmShjd3o84kbwG+CFxfVa9OmW+jTmkBzbXlT3Isk+DfWlV3Thtjo05pMc1ztj/AzcDuqvpMfyVJGsM8W/5fAH4D+KUku7rbZT3VJWlg83Tp/ScgPdYiaURe4Sc1yvBLjTL8UqMMv9Qowy81yvBLjTL8UqMMv9Qowy81yvBLjTL8UqMMv9Qowy81yvBLjTL8UqMMv9Qowy81yvBLjTL8UqMMv9SoucOf5Jgk30zy5T4KkjSOPrb81zHp0yfpKDJvx54NwK8BN/VTjqSxzLvl/xPg48D/9FCLpBHN067rcuClqnroMOOuTfJgkgdf54ezLk5Sz+Zt13VFkmeB25m07fqb5YNs1CktppnDX1U3VNWGqjoL2AJ8tao+2Ftlkgbl5/xSo2Zu1LlUVX0d+HofP0vSONzyS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNWredl0nJ7kjyZNJdid5Z1+FSRrWvN/e+6fA31fVB5IcB5zQQ02SRjBz+JOcBLwHuBqgql4DXuunLElDm2e3/2zgZeAvk3wzyU1J1vVUl6SBzRP+NcCFwF9U1QXA94GtywfZqFNaTPOEfy+wt6p2dI/vYPLH4A1s1Cktpnkadb4APJ/kvO6pTcATvVQlaXDznu3/beDW7kz/M8CH5i9J0hjmCn9V7QI29lSLpBF5hZ/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKj5m3U+btJHk/yWJLbkqztqzBJw5o5/EnOAH4H2FhV7wCOAbb0VZikYc27278G+PEka5h06P33+UuSNIZ5OvbsA/4IeA7YD7xSVff1VZikYc2z278e2MykW+/bgXVJPjhlnI06pQU0z27/LwP/WlUvV9XrwJ3Au5YPslGntJjmCf9zwMVJTkgSJo06d/dTlqShzXPMv4NJW+6dwKPdz9rWU12SBjZvo85PAJ/oqRZJI/IKP6lRhl9qlOGXGmX4pUYZfqlRhl9qlOGXGmX4pUYZfqlRhl9qlOGXGmX4pUYZfqlRhl9qlOGXGmX4pUYZfqlRhl9qlOGXGmX4pUYdNvxJPp/kpSSPLXnulCTbkzzd3a8ftkxJfVvNlv+vgEuXPbcVuL+qzgXu7x5LOoocNvxV9Y/Ad5Y9vRm4pZu+BXh/z3VJGtisx/ynVdX+bvoF4LSe6pE0krlP+FVVAbXSfBt1Sotp1vC/mOR0gO7+pZUG2qhTWkyzhv9u4Kpu+irgS/2UI2ksq/mo7zbgn4HzkuxN8mHgk8CvJHmaSavuTw5bpqS+HbZRZ1VducKsTT3XImlEXuEnNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UqFkbdX46yZNJHklyV5KThy1TUt9mbdS5HXhHVf0c8BRwQ891SRrYTI06q+q+qjrQPXwA2DBAbZIG1Mcx/zXAvT38HEkjOmzTjkNJciNwALj1EGOuBa4FWMsJ8yxOUo9mDn+Sq4HLgU1dp96pqmobsA3gJ3LKiuMkjWum8Ce5FPg48ItV9YN+S5I0hlkbdf45cCKwPcmuJJ8buE5JPZu1UefNA9QiaURe4Sc1yvBLjTL8UqMMv9Qowy81yvBLjTL8UqMMv9Qowy81yvBLjTL8UqMMv9Qowy81yvBLjTL8UqMMv9Qowy81yvBLjTL8UqMMv9SomRp1Lpn3sSSV5NRhypM0lFkbdZLkTOAS4Lmea5I0gpkadXb+mEnjDrvwSEehmY75k2wG9lXVwz3XI2kkb7pdV5ITgN9jssu/mvE26pQW0Cxb/p8BzgYeTvIssAHYmeRt0wZX1baq2lhVG4/l+NkrldSrN73lr6pHgbcefNz9AdhYVf/RY12SBjZro05JR7lZG3UunX9Wb9VIGo1X+EmNMvxSo1I13jU6SV4G/m2F2acCi3TScNHqgcWryXoO7UjU89NV9VOrGThq+A8lyYNVtfFI13HQotUDi1eT9RzaotWznLv9UqMMv9SoRQr/tiNdwDKLVg8sXk3Wc2iLVs8bLMwxv6RxLdKWX9KIRg9/kkuTfCvJniRbp8w/PskXuvk7kpw1YC1nJvlakieSPJ7kuilj3pvklSS7utvvD1XPkmU+m+TRbnkPTpmfJH/WraNHklw4YC3nLfm370ryapLrl40ZdB1N+zapJKck2Z7k6e5+/Qqvvaob83SSqwas59NJnuzej7uSnLzCaw/53o6qqka7AccA3wbOAY4DHgbOXzbmt4DPddNbgC8MWM/pwIXd9InAU1PqeS/w5ZHX07PAqYeYfxlwLxDgYmDHiO/fC0w+Sx5tHQHvAS4EHlvy3B8CW7vprcCnprzuFOCZ7n59N71+oHouAdZ005+aVs9q3tsxb2Nv+S8C9lTVM1X1GnA7sHnZmM3ALd30HcCmJBmimKraX1U7u+nvAruBM4ZYVs82A39dEw8AJyc5fYTlbgK+XVUrXag1iJr+bVJLf09uAd4/5aW/Cmyvqu9U1X8B25nylXR91FNV91XVge7hA0z+q/tCGzv8ZwDPL3m8l/8bth+N6VbmK8BPDl1Yd3hxAbBjyux3Jnk4yb1JfnboWph8Ndp9SR7qvgxludWsxyFsAW5bYd7Y6+i0qtrfTb8AnDZlzJFaT9cw2TOb5nDv7Wje9P/n//8oyVuALwLXV9Wry2bvZLKb+70klwF/B5w7cEnvrqp9Sd4KbE/yZLe1OWKSHAdcAdwwZfaRWEc/UlWVZCE+tkpyI3AAuHWFIQvz3o695d8HnLnk8YbuualjkqwBTgL+c6iCkhzLJPi3VtWdy+dX1atV9b1u+h7g2KG/qryq9nX3LwF3MTlcWmo167Fv7wN2VtWLy2cciXUEvHjwUKe7f2nKmFHXU5KrgcuBX6/uAH+5Vby3oxk7/N8Azk1ydrcl2QLcvWzM3cDBs7IfAL660oqcV3cu4WZgd1V9ZoUxbzt4ziHJRUzW2ZB/jNYlOfHgNJMTSct7JtwN/GZ31v9i4JUlu8BDuZIVdvnHXkedpb8nVwFfmjLmK8AlSdZ3nwZc0j3XuySXMvk26yuq6gcrjFnNezuesc8wMjlT/RSTs/43ds/9AZOVBrAW+FtgD/AvwDkD1vJuJsdgjwC7uttlwEeAj3RjPgo8zuSTiQeAdw28fs7plvVwt9yD62hpTQE+263DR5l8jdqQNa1jEuaTljw32jpi8kdnP/A6k+P2DzM5D3Q/8DTwD8Ap3diNwE1LXntN97u0B/jQgPXsYXJ+4eDv0cFPrN4O3HOo9/ZI3bzCT2qUV/hJjTL8UqMMv9Qowy81yvBLjTL8UqMMv9Qowy816n8Bn2y0w9hZADgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = env1.reset()\n",
    "costs = []\n",
    "done = False\n",
    "\n",
    "indices = []\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    mask = env1.available_actions.copy()\n",
    "\n",
    "    a, dist = algo.actor.forward(\n",
    "        to_tensor(s[None, :, :, None]),\n",
    "        mask=to_tensor(mask[None, :]),\n",
    "        with_dist=True)\n",
    "    a = a.cpu().detach().numpy()[0]\n",
    "    indices.append(a)\n",
    "\n",
    "    s, cost, done = env1.step(a)\n",
    "    costs.append(cost)\n",
    "    plt.imshow(s)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "print (np.sum(costs), np.max(costs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [env1.row_to_node[idx] for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 3, 1, 4, 9, 8, 7, 2, 5, 6]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5: 1, 11: 4, 0: 7, 2: 10, 4: 3, 9: 2, 7: 5, 8: 8, 10: 6, 6: 9}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env1.row_to_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 1, 2, 1, 2, 3, 2, 3, 2]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_peo??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(critic_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.graph_model as gm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.get_treewidth_from_peo(env2.graph, nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opensim",
   "language": "python3",
   "name": "opensim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
